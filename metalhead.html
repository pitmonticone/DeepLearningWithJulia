<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Logan Kilpatrick" />
  <title>Metalhead.jl - Deep Learning with Julia</title>
  <link rel="shortcut icon" type="image/png" href="/favicon.png"/>
  <link rel="stylesheet" href="/style.css"/>
    <script src="/mousetrap.min.js"></script>
    <!-- TODO: Add url_prefix. -->
  <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("/JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="/github.min.css">
<script src="/highlight.min.js"></script>
<script src="/julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        hljs.highlightElement(el);
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="/">Deep Learning with Julia</a>
</div><br />
<span class="books-subtitle">
using Flux.jl
</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="/about"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="/preface"><b>2</b> Preface</a></li>
<li><a class="menu-level-2" href="/why_deep_learning"><b>2.1</b> Why Deep Learning?</a></li>
<li><a class="menu-level-2" href="/book_motivation"><b>2.2</b> Why does this book exist?</a></li>
<li><a class="menu-level-2" href="/acknowledgements"><b>2.3</b> Acknowledgements</a></li>
<li><a class="menu-level-1" href="/why_julia"><b>3</b> Why Julia</a></li>
<li><a class="menu-level-2" href="/dispatch"><b>3.1</b> Multiple Dispatch</a></li>
<li><a class="menu-level-2" href="/packages"><b>3.2</b> Package Management</a></li>
<li><a class="menu-level-2" href="/community"><b>3.3</b> The Julia Community</a></li>
<li><a class="menu-level-1" href="/transfer_learning"><b>4</b> Transfer Learning</a></li>
<li><a class="menu-level-2" href="/pre-trained_models"><b>4.1</b> Pre-trained Models</a></li>
<li><a class="menu-level-2" href="/metalhead"><b>4.2</b> Metalhead.jl</a></li>
<li><a class="menu-level-1" href="/appendix"><b></b> Appendix</a></li>
<li><a class="menu-level-1" href="/references"><b></b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="4.2" id="sec:metalhead"><span class="header-section-number">4.2</span> Metalhead.jl</h2>
<p>Let us start out by installing Metalhead in the package manager by doing <code>add Metalhead</code>. Then we can type <code>using Flux, MetalHead</code> into a Julia terminal session. Metalhead provides a number of different model types like <code>resnet</code>, <code>vgg</code>, and more. Over the course of your deep learning experience, you will become more familiar with these model types as they represent some of the most common models for transfer learning. Before we dive into actually using pre-trained models, we will first take a quick look at the model structure which we get from Metalhead and Flux.</p>
<pre class="julia"><code>julia&gt; model = ResNet50(pretrain=false)
ResNet(
  Chain(
    Chain(
      Conv((7, 7), 3 =&gt; 64, pad=3, stride=2),  # 9_472 parameters
      BatchNorm(64, relu),              # 128 parameters, plus 128
      MaxPool((3, 3), pad=1, stride=2),
      Parallel(
        Metalhead.var&quot;#18#20&quot;(),
        Chain(
          Conv((1, 1), 64 =&gt; 64, bias=false),  # 4_096 parameters
          BatchNorm(64, relu),          # 128 parameters, plus 128
          Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  # 36_864 parameters
          BatchNorm(64, relu),          # 128 parameters, plus 128
          Conv((1, 1), 64 =&gt; 256, bias=false),  # 16_384 parameters
          BatchNorm(256),               # 512 parameters, plus 512
        ),
        Chain(
          Conv((1, 1), 64 =&gt; 256, bias=false),  # 16_384 parameters
          BatchNorm(256),               # 512 parameters, plus 512
        ),
      ),
      ...
      ...
      ...
    Chain(
      AdaptiveMeanPool((1, 1)),
      Flux.flatten,
      Dense(2048, 1000),                # 2_049_000 parameters
    ),
  ),
)         # Total: 162 trainable arrays, 25_557_096 parameters,
          # plus 106 non-trainable, 53_120 parameters, summarysize 97.749 MiB.</code></pre>
<p>Now that is a lot to take in (even though most of the model has been omitted for space reasons), but at a high level, it represents the structure of the <code>ResNet50</code> model as defined in Flux. Notably, the <code>50</code> in <code>ResNet50</code> comes from the fact that the model has 50 layers. There are other ResNet like models with different numbers of layers but with the same overall structure.</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="/pre-trained_models"><b>4.1</b> Pre-trained Models</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-1" href="/appendix"><b></b> Appendix</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Logan Kilpatrick
</div>
</div>
</div>
</body>
</html>